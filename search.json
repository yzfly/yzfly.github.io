[{"url":"/2019/03/03/Machine-Learning-1/","content":"# Machine Learning-1\n\ngoogle 机器学习速成教程笔记\n\n[机器学习速成课程]: https://developers.google.cn/machine-learning/crash-course/\n\n<!-- more -->\n\n## 1、术语：\n\n下面以对电子邮件数据处理为例子\n\n* **标签**： 我们要预测的真实事物-y，邮件为垃圾邮件还是正常邮件\n\n* **特征**：用于描述数据的输入变量 $x_i$，eg：邮件的各个维度特征\n\n  * 多维度特征按如下方式指定： $\\{x_1,x_2,...x_N\\}$ \n  * 实用特征： 可观察、可量化的指标。 不可量化的比如美观程度等不适合作为特征\n\n* **样本**：数据的特定实例。 eg:  一封邮件\n\n  * 有标签样本：具有｛特征，标签｝，即｛x,y｝的数据，既包含特征又包含标签\n    * 用于训练模型\n  * 无标签样本： 只具有｛特征，？｝，即｛x，？}的数据，只包含特征不包含标签\n    * 用于验证模型或者对新数据做出预测\n\n* **模型**： 定义了特征与标签之间的关系。可将样本映射到预测标签： $y^{’}$ 。下面是模型生命周期的两个阶段：\n\n  > 由模型的内部参数来定义，该模型的内部参数是由学习获得\n\n  * 训练：创建或者学习模型。向模型展示有标签样本，让模型学习特征与样本之间的关系。\n  * 推断：将训练后的样本应用与无标签样本。\n\n* **回归与分类：** \n\n  * 回归模型可预测连续值——比如预测概率\n  * 分类模型可预测离散值\n\n## 2、深入了解机器学习\n\n### 线性回归\n\n对蟋蟀叫声频率和温度之间的关系建立模型：\n$$\ny = mx+b\n$$\n在机器学习中，习惯使用下面的表示方法：\n$$\ny^{'} =b+w_1x_1+w_2x_2+w_3x_3\n$$\n其中：\n\n* $y^{'}$ 是指预测标签(理想输出值)\n* $b$ 指的是偏差，y轴截距，有的地方也使用$w_0$ 表示\n* $w_i$ 表示的是特征1的权重。权重与“斜率”这一概念表示的意思相近\n* $x_i$ 指的是特征，即已知输入项\n\n### 训练与损失\n\n**损失** 是一个数值表示对于单个样本而言模型预测的准确程度。模型的预测100%准确，则模型的损失为0，否则损失会比较大。训练模型的目标是从所有样本中找到一组使得损失函数平均损失“较小”的权重和偏差。\n\n**eg:** 在蟋蟀的例子中，损失为单个样本点到拟合直线的距离，总损失表征了模型的优劣。\n\n\n\n**常用损失函数-平方损失** ：\n\n* 平方损失在线性回归模型中使用频繁，又称为$L_2$ 损失，单个样本的平方损失为 $(y-y_i)^2$\n\n* 均方误差（MSE）指的是每个样本的平均平方损失。MSE的计算方法为，求出各个样本的所有平方损失之和，除以样本量：\n  $$\n  MSE = \\frac{1}{N}\\sum_{(x,y)\\in{D}}(y-y^{’}(x))\n  $$\n\n  * 其中 (x,y) 指的是样本\n    * x 指的是模型进行预测时使用的特征集\n    * y指的是样本的标签\n  * $y^{’}(x) $ 指的是权重和偏差与特征集 x结合的函数\n  * D 指的是多个包含有标签样本( 即 (x,y) )的数据集\n  * N指的是D中样本的数量\n\n### 降低损失(Reducing Loss)\n\n最佳模型是使得损失函数值最小的模型。\n\n通过计算整个数据集中 w1 每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。 \n\n回归问题产生的损失与权重图为凸型，凸型问题只有一个最小值点，这个最小值就是损失函数收敛之处。下面使用线性回归问题来举例说明\n\n#### **梯度下降法**\n\n为了寻找使得损失函数最小的模型参数，对建立的损失函数(自变量为模型参数)寻找极小值点，寻找方法为计算迭代点的梯度，改变模型参数使损失函数沿负梯度方向前进。通常，可以不断进行迭代，直到总体损失不再变化或者变化极其缓慢为止，此时我们说该模型已收敛。\n\n> 梯度为一个矢量： 具有方向和大小两个特征。梯度始终朝着损失函数增长最迅猛的方向，同理，负梯度方向即为损失函数减少最迅猛的方向。梯度下降算法会沿着负梯度的方向走一步，以便尽快降低损失。\n\n**起点：** 梯度下降法的第一个阶段是为 w1选择一个起始值，称为起点。起点并不重要，因此很多算法直接将起点设置为0或随机选择一个值。\n\n**下一个点：** 梯度下降算法依赖与负梯度。为了确定损失函数曲线上的下一个点，梯度下降算法将梯度大小的一部分与起点相加，一个梯度步长将迭代点移动到损失曲线上的下一个点，如下图所示：\n\n![1532479162926](F:\\Docs\\git\\study\\md\\ML\\media\\TD.png)\n\n机器学习算法用于训练模型的迭代试错过程如下图所示：\n\n![](F:\\Docs\\git\\study\\md\\ML\\media\\ML-try.png)\n\n在模型训练过程中，可以在每一步骤上计算整个数据集的梯度。但如此计算量太大，实际运用中计算小型数据样本的梯度效果就已经很好。所以实际可使用下面的方法降低损失。\n\n#### SGD和小批量梯度下降法\n\n* 随机梯度下降法： 依次抽取一个样本\n* 小批量梯度下降法： 每批包含 10~1000个样本\n  * 损失和梯度在整体范围内达到平衡\n\n#### 学习速率\n\n学习速率也称为步长。在梯度下降算法中，使用  梯度*学习速率 来确定下一个点的位置。即\n$$\nx_{i+1} = x_i+k*grad(x_i)\n$$\n其中 $x_i$ 为当前点位置，$x_{i+1}$ 为下一个点位置，$k$ 为学习速率，$grad(x_i)$ 为当前点的梯度。\n\n* 学习速率不宜过大也不宜过小，过小会导致学习时间过长，过大容易导致学习过程不收敛。如果已知损失函数的梯度较小，则采用较大的学习速率，如梯度较大，则采用较小的学习速率。\n\n#### 随机梯度下降法(SGD)\n\n大数据集或者超大数据集中，常常使用批量梯度下降法。**批量** 指的是在单次迭代中用于计算梯度的样本总数。\n\n包含随机抽样样本的大型数据集可能包含冗余数据。批量越大，出现冗余的可能性越高。为了估算出损失函数的平均梯度(对整个数据集而言)，同时减小计算量，可以通过从我们的数据集中随机选择样本，通过随机选择的小样本集来估算出总体的平均值。**SGD**将这种方法运用到极致，每次迭代只使用一个样本。\n\n**小批量SGD**是介于全批量SGD和SGD之间的折衷方案，小批量一般包含10~1000个随机选择的样本。既减小了SGD中的杂乱样本的数量，又提高了效率。\n\n"},{"title":"Hexo + GitPages + yuque 博客部署","url":"/2019/03/03/cw4qlh/","content":"使用github 提供的静态页面功能，结合优秀的hexo框架，搭建属于自己的静态博客。对于习惯语雀进行写作和文档管理的同学，搭配 hexo-yuque 插件，同步语雀内容至博客。\n\n若只需搭建个人博客，无需语雀同步功能，则涉及语雀部分均忽略即可。\n\n<a name=\"e94b83b8\"></a>\n## 安装准备\n* node 已安装并且为最新版\n* github 注册账号一个 \n* git 已安装并已经配置 github 访问\n* 注册语雀并建立知识库\n\n<a name=\"e6bd56c9\"></a>\n## Hexo 安装及配置\n\n* 安装node\n\n* 安装hexo 脚手架\n```bash\nnpm install -g hexo-cli\n```\n\n* 创建博客文件夹并进行初始化\n```bash\nmkdir blog\nhexo init blog\ncd blog\nnpm install \n```\n\n* 修改配置\n```bash\nvi _config.yml\n```\n\n修改部署部分，**注意冒号后需要有空格！！！这是语法**\n> deploy:\n>  type: git\n>  repo: git@github.com:yzfly/yzfly.github.io.git\n>  branch: master\n\n\n<a name=\"e31cad26\"></a>\n### 搜索功能\n为了使用搜索功能，首先需要安装下列插件：<br />npm i hexo-generator-search --save<br />然后在 _config.yml 中进行配置，可以参考如下配置：\n```\nsearch:\n  path: search.json\n  field: post\n```\n\n\n"}]